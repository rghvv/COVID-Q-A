{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from pyserini.search import pysearch\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import BertTokenizer, BartTokenizer, BartForConditionalGeneration, BertForQuestionAnswering\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How does weather, heat, and humidity affect the tramsmission of 2019-nCoV'\n",
    "keywords = ''\n",
    "hit_dict = {}\n",
    "\n",
    "searcher = pysearch.SimpleSearcher('lucene-index/')\n",
    "hits = searcher.search(query + '. ' + keywords, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exlporatory Pandas Stuff\n",
    "\n",
    "#ranks = range(1, len(hits)+1)\n",
    "#docids = [hit.docid for hit in hits]\n",
    "#scores = [hit.score for hit in hits]\n",
    "#titles = [ hit.lucene_document.get('title') for hit in hits]\n",
    "#dois = [ hit.lucene_document.get('doi') for hit in hits]\n",
    "#data = {'rank': ranks, 'docid': docids, 'score': scores, 'title': titles, 'doi': dois} \n",
    "#\n",
    "#df = pd.DataFrame(data)\n",
    "#print(df)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n",
      "dict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n",
      "dict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n",
      "dict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n",
      "dict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n",
      "dict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n",
      "dict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n",
      "dict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n",
      "dict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n"
     ]
    }
   ],
   "source": [
    "# Populate hit_dict from Lucene index\n",
    "\n",
    "for hit in hits:\n",
    "    key = hit.docid[:8]\n",
    "    json_result = searcher.doc(key).raw()\n",
    "    if len(json_result):\n",
    "        article = json.loads(searcher.doc(key).raw())\n",
    "        hit_dict[key] = {}\n",
    "        hit_dict[key]['title'] = article['metadata']['title']\n",
    "        hit_dict[key]['authors'] = article['metadata']['authors']\n",
    "        hit_dict[key]['doi'] = hit.lucene_document.get('doi')\n",
    "        hit_dict[key]['score'] = hit.score\n",
    "        hit_dict[key]['body'] = article['body_text']\n",
    "        if 'abstract' in article.keys():\n",
    "            if len(article['abstract']):\n",
    "                hit_dict[key]['abstract'] = article['abstract'][0]['text']\n",
    "        print(hit_dict[key].keys())\n",
    "\n",
    "# Each entry in hit_dict may or may not have an abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "    \t- Score: 13.340998649597168\n",
      "    \t- Title: Roles of meteorological conditions in COVID-19 transmission on a worldwide scale\n",
      "    \t- Abstract: The novel coronavirus (SARS-CoV-2/ 2019-nCoV) identified in Wuhan, China, in December 2019 has caused great damage to public health and economy worldw...\n",
      "    \t- Body: In the first season of 2020, an outbreak of atypical pneumonia (COVID-19) caused by a novel coronavirus (SARS-CoV2 or 2019-nCoV) 1 has spread all over the world and had a great impact on public health...\n",
      "    \t- DOI: 10.1101/2020.03.16.20037168\n",
      "    \t- Authors: ['Biqing Chen', 'Hao Liang', 'Xiaomin Yuan', 'Yingying Hu', 'Miao Xu', 'Yating Zhao', 'Binfen Zhang', 'Xuejun Zhu']\n",
      "\n",
      "2.\n",
      "    \t- Score: 14.552000045776367\n",
      "    \t- Title: Journal Pre-proof Investigation of effective climatology parameters on COVID-19 outbreak in Iran Investigation of Effective Climatology Parameters on COVID-19 Outbreak in Iran\n",
      "    \t- Abstract: SARS CoV-2 (COVID-19) Coronavirus cases are confirmed throughout the world and millions of people are being put into quarantine. A better understandin...\n",
      "    \t- Body: Since late December 2019, patients presenting with viral pneumonia due to an unidentified microbial agent were reported in Wuhan, China. It was an outbreak of the novel Coronavirus disease named 2019 ...\n",
      "    \t- DOI: 10.1016/j.scitotenv.2020.138705\n",
      "    \t- Authors: ['Mohsen Ahmadi', 'Abbas Sharifi', 'Shadi Dorosti', 'Jafarzadeh Saeid', 'Negar Ghoushchi', ' Ghanbari', 'Jafarzadeh Ghoushchi', 'Negar Ghanbari']\n",
      "\n",
      "3.\n",
      "    \t- Score: 13.37559986114502\n",
      "    \t- Title: Journal Pre-proof COVID-19 transmission in Mainland China is associated with temperature and humidity: A time-series analysis Title: COVID-19 transmission in Mainland China is associated with temperature and humidity: a time-series analysis\n",
      "    \t- Abstract: Journal Pre-proof J o u r n a l P r e -p r o o f 2 Title: COVID-19 transmission in Mainland China is associated with temperature and humidity: a time-...\n",
      "    \t- Body: J o u r n a l P r e -p r o o f 3 identified in Wuhan, China, has caused an outbreak of a novel coronavirus disease . Its typical clinical symptoms include fever, dry cough, myalgia, and pneumonia, and...\n",
      "    \t- DOI: 10.1016/j.scitotenv.2020.138778\n",
      "    \t- Authors: ['Hongchao Qi', 'Shuang Xiao', 'Runye Shi', 'Michael Ward', 'Yue Chen', 'Wei Tu', 'Qing Su', 'Wenge Wang', 'Xinyi Wang', 'Zhijie Zhang', '# ']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top search results\n",
    "\n",
    "i = 1\n",
    "for key in hit_dict.keys():\n",
    "    print(f\"{i}.\\n\\\n",
    "    \\t- Score: {hit_dict[key]['score']}\\n\\\n",
    "    \\t- Title: {hit_dict[key]['title']}\\n\\\n",
    "    \\t- Abstract: {hit_dict[key]['abstract'][:150] + '...' if 'abstract' in hit_dict[key].keys() else 'N/A'}\\n\\\n",
    "    \\t- Body: {hit_dict[key]['body'][0]['text'][:200]}...\\n\\\n",
    "    \\t- DOI: {hit_dict[key]['doi']}\\n\\\n",
    "    \\t- Authors: {[author['first'] + ' ' + author['last'] for author in hit_dict[key]['authors']]}\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hit_dict[key]:\n",
      "\t<class 'dict'>\n",
      "\tdict_keys(['title', 'authors', 'doi', 'score', 'body', 'abstract'])\n",
      "===\n",
      "\n",
      "hit_dict[key][\"title\"]:\n",
      "\t<class 'str'>\n",
      "===\n",
      "\n",
      "hit_dict[key][\"authors\"]:\n",
      "\t<class 'list'>\n",
      "\n",
      "\thit_dict[key][\"authors\"][author_index]:\n",
      "\t\t<class 'dict'>\n",
      "\t\tdict_keys(['first', 'middle', 'last', 'suffix', 'affiliation', 'email'])\n",
      "===\n",
      "\n",
      "hit_dict[key][\"doi\"]:\n",
      "\t<class 'str'>\n",
      "===\n",
      "\n",
      "hit_dict[key][\"score\"]:\n",
      "\t<class 'float'>\n",
      "===\n",
      "\n",
      "hit_dict[key][\"body\"]:\n",
      "\t<class 'list'>\n",
      "\n",
      "\thit_dict[key][\"body\"][paragraph_index]:\n",
      "\t\t<class 'dict'>\n",
      "\t\tdict_keys(['text', 'cite_spans', 'ref_spans', 'section'])\n",
      "===\n",
      "\n",
      "hit_dict[key][\"abstract\"]:\n",
      "\t<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Organization of hit_dict\n",
    "\n",
    "key = ''\n",
    "for i in hit_dict.keys():\n",
    "    if 'abstract' in hit_dict[i].keys():\n",
    "        key = i\n",
    "        break\n",
    "\n",
    "print(f'hit_dict[key]:\\n\\t{type(hit_dict[key])}\\n\\t{hit_dict[key].keys()}')\n",
    "\n",
    "print('===')\n",
    "print(f'\\nhit_dict[key][\"title\"]:\\n\\t{type(hit_dict[key][\"title\"])}')\n",
    "\n",
    "print('===')\n",
    "print(f'\\nhit_dict[key][\"authors\"]:\\n\\t{type(hit_dict[key][\"authors\"])}')\n",
    "print(f'\\n\\thit_dict[key][\"authors\"][author_index]:\\n\\t\\t{type(hit_dict[key][\"authors\"][0])}\\n\\t\\t{hit_dict[key][\"authors\"][0].keys()}')\n",
    "\n",
    "print('===')\n",
    "print(f'\\nhit_dict[key][\"doi\"]:\\n\\t{type(hit_dict[key][\"doi\"])}')\n",
    "\n",
    "print('===')\n",
    "print(f'\\nhit_dict[key][\"score\"]:\\n\\t{type(hit_dict[key][\"score\"])}')\n",
    "\n",
    "print('===')\n",
    "print(f'\\nhit_dict[key][\"body\"]:\\n\\t{type(hit_dict[key][\"body\"])}')\n",
    "print(f'\\n\\thit_dict[key][\"body\"][paragraph_index]:\\n\\t\\t{type(hit_dict[key][\"body\"][0])}\\n\\t\\t{hit_dict[key][\"body\"][0].keys()}')\n",
    "\n",
    "print('===')\n",
    "print(f'\\nhit_dict[key][\"abstract\"]:\\n\\t{type(hit_dict[key][\"abstract\"])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): EncoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): DecoderLayer(\n",
       "          (self_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): SelfAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "QA_MODEL = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "QA_TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "QA_MODEL.to(torch_device)\n",
    "QA_MODEL.eval()\n",
    "\n",
    "SUMMARY_TOKENIZER = BartTokenizer.from_pretrained('bart-large-cnn')\n",
    "SUMMARY_MODEL = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n",
    "SUMMARY_MODEL.to(torch_device)\n",
    "SUMMARY_MODEL.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBERTSQuADPrediction(document, question):\n",
    "    nWords = len(document.split())\n",
    "    input_ids_all = QA_TOKENIZER.encode(question, document)\n",
    "    tokens_all = QA_TOKENIZER.convert_ids_to_tokens(input_ids_all)\n",
    "    overlapFac = 1.1\n",
    "    if len(input_ids_all)*overlapFac > 2048:\n",
    "        nSearchWords = int(np.ceil(nWords/5))\n",
    "        quarter = int(np.ceil(nWords/4))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[quarter-int(nSearchWords*overlapFac/2):quarter+int(quarter*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[quarter*2-int(nSearchWords*overlapFac/2):quarter*2+int(quarter*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[quarter*3-int(nSearchWords*overlapFac/2):quarter*3+int(quarter*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        \n",
    "    elif len(input_ids_all)*overlapFac > 1536:\n",
    "        nSearchWords = int(np.ceil(nWords/4))\n",
    "        third = int(np.ceil(nWords/3))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[third-int(nSearchWords*overlapFac/2):third+int(nSearchWords*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[third*2-int(nSearchWords*overlapFac/2):third*2+int(nSearchWords*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "        \n",
    "    elif len(input_ids_all)*overlapFac > 1024:\n",
    "        nSearchWords = int(np.ceil(nWords/3))\n",
    "        middle = int(np.ceil(nWords/2))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), \n",
    "                     ' '.join(docSplit[middle-int(nSearchWords*overlapFac/2):middle+int(nSearchWords*overlapFac/2)]),\n",
    "                     ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "    elif len(input_ids_all)*overlapFac > 512:\n",
    "        nSearchWords = int(np.ceil(nWords/2))\n",
    "        docSplit = document.split()\n",
    "        docPieces = [' '.join(docSplit[:int(nSearchWords*overlapFac)]), ' '.join(docSplit[-int(nSearchWords*overlapFac):])]\n",
    "        input_ids = [QA_TOKENIZER.encode(question, dp) for dp in docPieces]\n",
    "    else:\n",
    "        input_ids = [input_ids_all]\n",
    "    absTooLong = False    \n",
    "    \n",
    "    answers = []\n",
    "    cons = []\n",
    "    for iptIds in input_ids:\n",
    "        tokens = QA_TOKENIZER.convert_ids_to_tokens(iptIds)\n",
    "        sep_index = iptIds.index(QA_TOKENIZER.sep_token_id)\n",
    "        num_seg_a = sep_index + 1\n",
    "        num_seg_b = len(iptIds) - num_seg_a\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "        assert len(segment_ids) == len(iptIds)\n",
    "        n_ids = len(segment_ids)\n",
    "\n",
    "        if n_ids < 512:\n",
    "            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds]).to(torch_device), \n",
    "                                     token_type_ids=torch.tensor([segment_ids]).to(torch_device))\n",
    "        else:\n",
    "            absTooLong = True\n",
    "            start_scores, end_scores = QA_MODEL(torch.tensor([iptIds[:512]]).to(torch_device),\n",
    "                                     token_type_ids=torch.tensor([segment_ids[:512]]).to(torch_device))\n",
    "        start_scores = start_scores[:,1:-1]\n",
    "        end_scores = end_scores[:,1:-1]\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "        answer = reconstructText(tokens, answer_start, answer_end+2)\n",
    "    \n",
    "        if answer.startswith('. ') or answer.startswith(', '):\n",
    "            answer = answer[2:]\n",
    "            \n",
    "        c = start_scores[0,answer_start].item()+end_scores[0,answer_end].item()\n",
    "        answers.append(answer)\n",
    "        cons.append(c)\n",
    "    \n",
    "    maxC = max(cons)\n",
    "    iMaxC = [i for i, j in enumerate(cons) if j == maxC][0]\n",
    "    confidence = cons[iMaxC]\n",
    "    answer = answers[iMaxC]\n",
    "    \n",
    "    sep_index = tokens_all.index('[SEP]')\n",
    "    full_txt_tokens = tokens_all[sep_index+1:]\n",
    "    \n",
    "    abs_returned = reconstructText(full_txt_tokens)\n",
    "\n",
    "    ans={}\n",
    "    ans['answer'] = answer\n",
    "    if answer.startswith('[CLS]') or answer_end.item() < sep_index or answer.endswith('[SEP]'):\n",
    "        ans['confidence'] = -1000000\n",
    "    else:\n",
    "        ans['confidence'] = confidence\n",
    "    ans['abstract_bert'] = abs_returned\n",
    "    ans['abs_too_long'] = absTooLong\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructText(tokens, start=0, stop=-1):\n",
    "    tokens = tokens[start: stop]\n",
    "    if '[SEP]' in tokens:\n",
    "        sepind = tokens.index('[SEP]')\n",
    "        tokens = tokens[sepind+1:]\n",
    "    txt = ' '.join(tokens)\n",
    "    txt = txt.replace(' ##', '')\n",
    "    txt = txt.replace('##', '')\n",
    "    txt = txt.strip()\n",
    "    txt = \" \".join(txt.split())\n",
    "    txt = txt.replace(' .', '.')\n",
    "    txt = txt.replace('( ', '(')\n",
    "    txt = txt.replace(' )', ')')\n",
    "    txt = txt.replace(' - ', '-')\n",
    "    txt_list = txt.split(' , ')\n",
    "    txt = ''\n",
    "    nTxtL = len(txt_list)\n",
    "    if nTxtL == 1:\n",
    "        return txt_list[0]\n",
    "    newList =[]\n",
    "    for i,t in enumerate(txt_list):\n",
    "        if i < nTxtL -1:\n",
    "            if t[-1].isdigit() and txt_list[i+1][0].isdigit():\n",
    "                newList += [t,',']\n",
    "            else:\n",
    "                newList += [t, ', ']\n",
    "        else:\n",
    "            newList += [t]\n",
    "    return ''.join(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchAbstracts(hit_dictionary, question):\n",
    "    abstractResults = {}\n",
    "    for k,v in tqdm(hit_dictionary.items()):\n",
    "        if 'abstract' in v.keys():\n",
    "            abstract = v['abstract']\n",
    "        else:\n",
    "            abstract = None\n",
    "        if abstract:\n",
    "            ans = makeBERTSQuADPrediction(abstract, question)\n",
    "            if ans['answer']:\n",
    "                confidence = ans['confidence']\n",
    "                abstractResults[confidence]={}\n",
    "                abstractResults[confidence]['answer'] = ans['answer']\n",
    "                abstractResults[confidence]['abstract_bert'] = ans['abstract_bert']\n",
    "                abstractResults[confidence]['idx'] = k\n",
    "                abstractResults[confidence]['abs_too_long'] = ans['abs_too_long']\n",
    "                \n",
    "    cList = list(abstractResults.keys())\n",
    "\n",
    "    if cList:\n",
    "        maxScore = max(cList)\n",
    "        total = 0.0\n",
    "        exp_scores = []\n",
    "        for c in cList:\n",
    "            s = np.exp(c-maxScore)\n",
    "            exp_scores.append(s)\n",
    "        total = sum(exp_scores)\n",
    "        for i,c in enumerate(cList):\n",
    "            abstractResults[exp_scores[i]/total] = abstractResults.pop(c)\n",
    "    return abstractResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "abstractResults = searchAbstracts(hit_dict, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the novel coronavirus (sars-cov-2 / 2019-ncov) identified in wuhan, china, in december 2019 has caused great damage to public health and economy worldwide with over 140,000 infected cases up to date. previous research has suggested an involvement of meteorological conditions in the spread of droplet-mediated viral diseases, such as influenza. however, as for the recent novel coronavirus, few studies have discussed systematically about the role of daily weather in the epidemic transmission of the virus. here, we examine the relationships of meteorological variables with the severity of the outbreak on a worldwide scale. the confirmed case counts, which indicates the severity of covid-19 spread, and four meteorological variables, i. e., air temperature, relative humidity, wind speed, and visibility, were collected daily between january 20 and march 11 (52 days) for 430 cities and districts. cc-by-nc-nd 4. 0 international license it is made available under a is the author / funder, who has granted medrxiv a license to display the preprint in perpetuity.\n"
     ]
    }
   ],
   "source": [
    "print(abstractResults[list(abstractResults.keys())[0]]['abstract_bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
